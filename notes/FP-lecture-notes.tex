\documentclass[a4paper,oneside]{memoir}


\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{lmodern}



%\usepackage[czech]{babel}
\usepackage{amsmath,amssymb,mathtools,amsthm,bm}

\usepackage{xspace}


\usepackage{lipsum}





\usepackage{qtree}
\usepackage[usenames,dvipsnames,table]{xcolor}
\usepackage{amsfonts}
% \usepackage{amsmath}
\usepackage{graphicx}

\usepackage[vlined,ruled,norelsize]{algorithm2e}




\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}


%\usepackage{enumitem}

\title{Some FP lecture notes (v0.6)}

\author{Tomáš Křen}

\hyphenation{vě-dec-ká}

\begin{document}

\theoremstyle{plain} 
\newtheorem{theorem}{Theorem} 
\newtheorem{proposition}{Proposition} 
\newtheorem{lemma}{Lemma} 
\newtheorem{preLemma}{Pre-Lemma} 
\newtheorem*{corollary}{Corollary}

\theoremstyle{definition} 
\newtheorem*{definition}{Definition} 
\newtheorem*{preDefinition}{Pre-Definition} 
\newtheorem{conjecture}{Conjecture}
\newtheorem*{example}{Example} 

\theoremstyle{remark} 
\newtheorem*{remark}{Remark} 
\newtheorem*{note}{Note} 
\newtheorem{case}{Case}

\frontmatter
\mainmatter
\maketitle

%\renewcommand{\chaptername}{Akt}

\tableofcontents*
%\clearpage

\newcommand{\red}[1]{{\color{red} #1}}



\newcommand{\sigmaPr}{\sigma^\prime}
\newcommand{\tauPr}{\tau^\prime}
\newcommand{\xPr}{x^\prime}
\newcommand{\nPr}{n^\prime}
\newcommand{\nPrr}{n^{\prime\prime}}
\newcommand{\nPrrr}{n^{\prime\prime\prime}}
\newcommand{\tausPr}{\tau_s^\prime}
\newcommand{\s}{\sigma}
\newcommand{\Th}{\theta}
\newcommand{\sPr}{\sigmaPr}
\newcommand{\thPr}{\theta^\prime}



\newcommand{\then}{\Rightarrow}
\newcommand{\E}[2]{(\exists #1)\ #2}
\newcommand{\A}[2]{(\forall #1)\ #2}
\newcommand{\Ain}[3]{(\forall #1 \in #2)\ #3}


\newcommand{\op}{\operatorname}

\newcommand{\ar}{\rightarrow}
\newcommand{\ap}[2]{(#1\,#2)}
\newcommand{\defi}{\coloneqq}
\newcommand{\defe}{\mathrel{\vcentcolon\equiv}}

\newcommand{\unaRule}[2]{\dfrac{#1}{#2}}
\newcommand{\binRule}[3]{\dfrac{#1 ~ ~ ~ ~ ~ ~ ~ #2}{#3}}
\newcommand{\triRule}[4]{\dfrac{#1 ~ ~ ~ ~ ~ ~ ~ #2 ~ ~ ~ ~ ~ ~ ~ #3}{#4}}
\newcommand{\isSub}[1]{#1\ \mathit{substitution}}
\newcommand{\MGU}[2]{\op{MGU}(#1,#2)}
\newcommand{\mgu}[1]{\op{MGU}(#1)}

\newcommand{\AX}{\textit{AX}\xspace}
\newcommand{\subAx}{\textit{SUB-AX}\xspace}
\newcommand{\mguMp}{\textit{MGU-MP}\xspace}
\newcommand{\abs}[1]{\lvert #1 \rvert}

\newcommand{\Pseudokod}[4]{
	\begin{figure}[!t]
	\removelatexerror
	\begin{algorithm}[H]
		\caption{\label{#4}#1}
		\DontPrintSemicolon
		\SetKwProg{Fn}{function}{}{}
		\Fn{#2}{#3}
	\end{algorithm}
	\end{figure}
}

\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

\newcommand{\la}{\leftarrow\xspace}





\newcommand{\inhab}[1]{\op{I}(#1)}

\newcommand{\tord}{\preccurlyeq}
\newcommand{\stord}{\prec}
\newcommand{\ordt}{\tord_\tau}
\newcommand{\tek}{\sim}
\newcommand{\ntek}{\nsim}
\newcommand{\ekt}{\tek_\tau}
\newcommand{\nekt}{\ntek_\tau}
\newcommand{\nsucct}{\nsucc_\tau}

\newcommand{\MGI}[1]{\op{MGI}(#1)}
\newcommand{\MGIt}{\MGI{\tau}}
\newcommand{\It}{\op{I}(\tau)}

\newcommand{\ids}{\sigma_{\op{id}}}

\newcommand{\U}[2]{\op{U}(#1,#2)}
\newcommand{\Utt}{\U{\tau}{\tauPr}}
\newcommand{\MGUtt}{\MGU{\tau}{\tauPr}}

\newcommand{\e}[2]{\op{E}_{#1}(#2)}
\newcommand{\restrict}[2]{{#1}_{\mid #2}}
\newcommand{\fresh}[2]{\op{fresh}_{#1}(#2)}
\newcommand{\newVar}[1]{\op{newVar}(#1)}
\newcommand{\Ss}[1]{\op{ss}(#1)}
\newcommand{\TS}[2]{\op{ts}_{#1}(#2)}
\newcommand{\ts}[2]{\op{ts}_{#1}(#2)}
\newcommand{\TSij}[3]{\op{ts}_{#1,#2}(#3)}
\newcommand{\trees}[2]{\op{trees}_{#1}(#2)}
\newcommand{\FX}{\ap{F}{X}}
\newcommand{\sF}{\s_{F}}
\newcommand{\sX}{\s_{X}}
\newcommand{\vars}[1]{\op{vars}(#1)}
\newcommand{\dom}[1]{\op{dom}(#1)}
\newcommand{\IH}{induction hypothesis\xspace}
\newcommand{\discup}{~\mathbin{\dot{\cup}}~}

\newcommand{\Real}{\mathbb{R}}

\newcommand{\letin}[3]{\texttt{let} \, #1 = #2 \, \texttt{in} \, #3}
\newcommand{\W}{\op{\textbf{W}}}
\newcommand{\Mgu}{\op{\textbf{MGU}}}
\newcommand{\where}{\op{\textbf{where~}}}
\newcommand{\SPr}{S^\prime}


\chapter{Hindley-Milner Type System}

\red{The text is not completely finished, but hopefully the sections dealing with the Hindley-Milner algorithm $\W$ are sufficiently understandable.}


\section*{TODO list}

\red{
\begin{itemize}
\item Define rest of the context stuff
\item Textually describe unification algorithm
\item In W comment for each case what is going on.
\item write Intro
\end{itemize}
}

\section{Introduction}

\red{Todo: Why Hindley-Milner? (= simplified system F capable of type inference in curry style).}

\section{Type Language}
\label{sec:typelang}

\red{TODO probably replace simple-type with mono-type}

A type system connects a type language with a program language.
In this section we present the type language (i.e. what are legal type expressions and what are they supposed to mean) used in Hindley-Milner type system.

We start with a subset of the language, \textit{simple-types}.
The whole language is obtained by enriching the simple-types with polymorphism.

Language of simple-types consists of tree type constructs, a simple-type is either: 
a type \textit{symbol}, 
a type \textit{term}, 
or a type \textit{variable}.
Let us have a closer look on each of the constructs.

A~\textit{type symbol} is a symbol of a specific basic type, such as 
$Int$, $Bool$, $String$, or $\Real$ (which are all inhabited by values); 
or a symbol of a specific parametric type such as $List$ or $\ar$ 
which are not inhabited. These uninhabited symbols serve as "functions over types" 
and need one or more types as type parameters. For example, in $(List~Int)$
or $(Int \ar Bool)$, we need to know what elements does the list have, 
or what is the function's domain and co-domain. Type symbol is written as 
a single capitalized word.

A~\textit{type term} is a construct for creating compound types given 
by a sequence of types, e.g., $(List~Int)$, or $(Int \ar Bool)$. 
Type term construct is a basis for construction of \textit{parametric} types,
which are generally written as a parenthesized sequence of type expressions, 
i.e. $(\tau_1~\tau_2~\dots~\tau_n)$, where $\tau_i$ are arbitrary 
simple-type expressions.

A~\textit{type variable} is a construct representing a type which has not been fully
specified yet. For instance, $\alpha$, $\beta_1$ and $\beta_2$ are type variables.
Type variables form \textit{polymorphic} types, 
e.g. $(List~\alpha)$, standing for a list of every (not yet specified) type. For instance,
this list is inhabited by an empty list, $[~] : (List~\alpha)$.
Other example of polymorphic type includes $(\alpha \ar \alpha)$ which stands for an unary 
operation over every type, for instance inhabited by
an identity function $id : \alpha \ar \alpha$. 
But not all polymorphic types are inhabited, e.g., there is no reasonable program having 
type $(\alpha \ar \beta)$ since there is no function such that it is from any type to any other 
type. 
In this text, type variables are written as Greek letters from the beginning of the alphabet ($\alpha$ and $\beta$ with possible index, e.g. $\beta_{42}$); 
in Haskell a type variable is written as a single lowercase word, usually one character long.

We get the full type language by adding \textit{poly-types} (from \textit{polymorphic}). Poly-types  are created by use of the fourth type construct: the universal quantification using $\forall$. 

More specifically we can take any type expression $\sigma$ (simple-type or poly-type) and make 
it a poly-type by prefixing it with a quantification of one type variable (which usually appears inside $\sigma$). If the quantified type variable is $\alpha$, then the newly created poly-type expression is $\forall \alpha . \sigma$. All the occurrences of $\alpha$ inside $\forall \alpha . \sigma$ are said to be \textit{bound}, as opposed to \textit{free}.

% Roughly speaking, when a type variable is quantified in a poly-type it means that any type can be plugged inside instead of this    

Here is a subtle and confusing yet very important difference between the way how polymorphic types 
are written in Haskell type language and in Hindley-Milner type language, simply said:

~

\textbf{In Haskell all the type variables in polymorphic types are implicitly quantified.} 

~

For example the \texttt{map} function has Haskell type \texttt{(a -> b) -> [a] -> [b]},
or we can pretend that it is \texttt{(a -> b) -> (List a) -> (List b)} to be compatible with our type term notation.
And \texttt{map} in Haskell with this type is polymorphic in both its type variables \texttt{a}
and \texttt{b}. So if we need to translate this Haskell type to Hindley-Milner type, we must prepend a quantification for each type variable occurring in the type:

$\forall a . (\forall b . (a \ar b) \ar (\op{List}~a) \ar (\op{List}~b))$

Which is abbreviated (similarly as with lambda abstractions) as:

$\forall a\,b\,.\,(a \ar b) \ar (\op{List}~a) \ar (\op{List}~b)$

Or even closer to our conventions as (because when something is bounded we can rename in the similar fashion as with lambda abstractions bounding program variables):

$\forall \alpha\,\beta\,.\,(\alpha \ar \beta) \ar (\op{List}~\alpha) \ar (\op{List}~\beta)$

~

So what is the purpose of free type variables?

We may say that in Hindley-Milner algorithm the type variables are used for two different purposes, depending on whether they are free or bound by $\forall$ quantifier.

The bound variables act as we know them from Haskell, the tricky part are the free variables.

\red{TODO specifiy the convetions regarding $\sigma$ for poly-types (well more precisely any-type) and  $\tau$ for mono-types}


We demonstrate the difference 
on an example type $\tau = (\alpha \ar \beta)$ and $\sigma = (\forall \alpha \beta . \alpha \ar \beta)$:
\begin{itemize}
\item There is no program expression $e$ such that $e : \forall \alpha \beta . \alpha \ar \beta$, because it would be a polymorphic function magically converting values between every pair of types.
\item On the other hand statement $e : \alpha \ar \beta$ tells us only that $e$ is some function, we don't know yet, which specific types will stand for $\alpha$ and $\beta$. It corresponds to a more intermediate result.    

\red{TODO: formulate more precisely...}
\end{itemize}





\section{Type Substitutions}

Generally speaking, a substitution is a function used for replacement 
of variables in an expression by some other expression.
Substitution is used both in program expressions 
(e.g. it is the core of $\beta$-reduction) and type expressions.
Here we will be dealing with substitutions on the type level. 

A type substitution is a finite mapping from type variables to types.
It is usually denoted as a collection of $key \mapsto value$ pairs.
The general form is:

$$\{ \alpha_1 \mapsto \tau_1, \alpha_2 \mapsto \tau_2, \dots, \alpha_n \mapsto \tau_n \}$$ 

For example, let 

$S = \{ \alpha_3 \mapsto \op{Int}, 
\beta_3 \mapsto (\op{List}~\alpha_6),
\alpha_5 \mapsto \beta_1, 
\beta_{23} \mapsto (\alpha_3 \ar \beta_1) \}$ 
and 

$\tau = ((\alpha_3 \ar \alpha_5) \ar ( \beta_1  \ar (\op{List}~\alpha_6) ) )$,
then 

$\tauPr = S(\tau) = ((\op{Int} \ar \beta_1) \ar (\beta_1  \ar (\op{List}~\alpha_6) ) )$.

Generally, by applying a substitution $S$ to type $\tau$, we get a \textit{more specific} type $\tauPr = S(\tau)$.

A special (but often seen, e.g. in Hindley-Milner algorithm) case of substitution is an
empty substitution, denoted as $\{\}$, having no effect when applied; 
i.e. $\{\}(\tau) = \tau$.


Because substitutions can be dealt with as with functions, we can compose them using composition operator $\circ$. Let $R = S_2 \circ S_1$, then 
$R(\tau) = (S_2 \circ S_1)(\tau) = S_2 ( S_1(\tau) )$. 



\section{Typing Contexts}

\begin{definition}
A $\mathit{term:type}$ statement $\mathit{M}:\mathit{\tau}$ states that (program) term $M$ has type $\tau$.   
A \textit{declaration} is a statement $s : \tau$ where $s$ is a term symbol and $\tau$ is a type.
A \textit{context} is set of declarations with distinct term symbols.\footnote{Interestingly, the definition of a \textit{context} and definition of a \textit{substitution} are almost the same. The difference is that "keys" in a context are term symbols/variables, whereas substitution "keys" are type variables. Maybe this fact could be utilized in an interesting way...}
\end{definition}

\red{def $\Gamma_x$} ; \red{def $\overline{\Gamma}(\tau)$}



\section{Hindley-Milner Algorithm W}

The Hindley-Milner algorithm $\W$ is used for type inference.
Loosely speaking, we give to $\W$ as an input 
a \textit{program expression} $e$ without type information 
and it returns a \textit{type} $\tau$ of that expression as a result, 
or it tells us that the expression cannot be typed correctly.

From this simplified point of view we may see the algorithm usage as:

(1) We have an expression $e$, for which we would like to know the type. 

So we run $\W$ on $e$ and we may either get as a result:

(2a) a type $\tau$, so we know that $e$ has type $\tau$,

(2b) or the \textit{fail result} $\bot$ (usually called \textit{bottom}), so we know there is a type error inside $e$.

~   

The first simplification of this description lies in that we have omitted 
the typing contexts (the "Gammas"). 
All the inference rules deal with \textit{judgments} of the form:

$$\Gamma \vdash e : \tau$$

And so does the $\W$ algorithm.
If $e$ is the top-level program expression,
we can think of a context $\Gamma$ as a collection of type information about the "library"
in which the program expression $e$ is written.
Or, if $e$ is some local sub-expression, then its $\Gamma$ contains also type information about
all the local variables defined in its scope.

We can think of a judgment of the form $\Gamma \vdash e : \tau$ as: 
\textit{From the building symbols described in the typing context $\Gamma$ we can build 
a well-typed program expression $e$ which has type $\tau$.}
Therefore it makes sense to provide a typing context $\Gamma$ to the $\W$ algorithm as another argument: $\W(\Gamma, e)$.

But $\W$ algorithm is even stronger: We may use libraries for which we do not know the proper typing information yet.

For example consider the following expression $e$:

$$ \lambda x . ((+~((+~x)~1))~x) $$

Or, in a more readable fashion, $e = \lambda x . (x+1)+x $.


And let's pretend that the only thing we know is that $1 : \op{Int}$, 
but we don't know the type of $+$. 
$\W$ can deal with this situation and infer that $e$ has type $Int \ar Int$ and
that $+$ has type $Int \ar Int \ar Int$. This can be achieved by calling $\W$ with
typing context $\Gamma = \{ 1 : \op{Int}, + : \alpha \}$, 
where $\alpha$ is a \textit{type variable}.

But if the only result of the $\W(\Gamma,e)$ is the type $\tau$ of $e$, 
how we get the information about the inferred type of $+$? 
Well, $\W$ actually returns a pair $(S, \tau)$, where $S$ is a substitution
containing the rest of the inferred type information. 
More specifically, $S(\alpha) = Int \ar Int \ar Int$.  

Now we can state the behavior of the $\W$ algorithm more formally:

~

Given context $\Gamma$ and expression $e$ the Hindley-Milner algorithm $\W$ 
is looking for the substitution $S$ and type $\tau$ such that: 
$$ S(\Gamma) \vdash e : \tau $$
If there are no such $S$ and $\tau$, then the $\W$ algorithm fails.
But if there are any, $\W$ finds the most general $S$ a $\tau$.


\begin{align*}
\W(\Gamma, e) = &
\begin{cases*}
  (S, \tau) 
  & \textbf{if} there is any $\SPr$ and $\tauPr$ such that $S^\prime(\Gamma) \vdash e : \tauPr$  \\
  \bot & \textbf{otherwise}
\end{cases*}\\
\end{align*}




\subsection{Definition of W algorithm}

Here we present a recursive definition of $\W$ algorithm based on case analysis of all possible patterns that program expression $e$ may have 
(i.e. $e$ may be a \textit{variable}, an \textit{application}, an \textit{abstraction}, or a \textit{let-expression}). 

~

Whenever there is a type variable $\beta$ (or $\beta_i$) it is meant to be a \textit{new fresh type variable}, that has not occurred anywhere in $\Gamma$ or during the computation of some other sub-expression. Namely, fresh type variables $\beta$ are introduced in cases \textbf{(1)}, \textbf{(2)} and \textbf{(3)}. 

During the computation of cases \textbf{(2)}, \textbf{(3)} and \textbf{(4)} there is a recursive call to $\W$ which can possibly fail; whenever a recursive call fails, then also the calling computation fails.

~

\textbf{(1)} Expression $e$ is a \textit{variable}; $e = x$:

\begin{align*}
\W(\Gamma, x) \defi ~ &
\begin{cases*}
  (\{\}, R(\tauPr) ) 
  & \textbf{if} $(x : \forall \alpha_1 \dots \alpha_n.\tauPr) \in \Gamma$  \\
  \bot & \textbf{otherwise}
\end{cases*}\\
\where & R = \{\alpha_1 \mapsto \beta_1, \dots, \alpha_n \mapsto \beta_n\} \\
\end{align*}


\textbf{(2)} Expression $e$ is an \textit{application}; $e = (e_1~e_2)$:

\begin{align*}
\W(\Gamma, (e_1~e_2)) \defi ~ & 
\begin{cases*}
  (R \circ S_2 \circ S_1, R(\beta) ) & \textbf{if} $R \neq \bot$ \\
  \bot & \textbf{if} $R = \bot$
\end{cases*}\\
\where & (S_1, \tau_1) = \W(\Gamma, e_1), \\
       & (S_2, \tau_2) = \W(S_1(\Gamma), e_2), \\
       & R = \Mgu( S_2(\tau_1), \tau_2 \ar \beta ).\\
\end{align*}


\textbf{(3)} Expression $e$ is an \textit{abstraction}; $e = \lambda x . e_1$:

\begin{align*}
\W(\Gamma, \lambda x .e_1) \defi ~ & (S_1, S_1(\beta) \ar \tau_1 ) \\
\where & (S_1, \tau_1) = \W(\Gamma_x, x : \beta ~;~ e_1). \\
\end{align*}



\textbf{(4)} Expression $e$ is a \textit{let-expression}; $e = (\letin{x}{e_1}{e_2})$:

\begin{align*}
\W(\Gamma, \letin{x}{e_1}{e_2}) \defi ~ & (S_2 \circ S_1, \tau_2) \\
\where & (S_1, \tau_1) = \W(\Gamma, e_1), \\
       & (S_2, \tau_2) = \W(S_1(\Gamma_x),x:\overline{(S_1(\Gamma))}(\tau_1); e_2). \\
\end{align*}

\subsection{Example run of W algorithm}

We demonstrate $\W$ algorithm on simple example which contains all four possible forms of expressions as sub-expressions:
$$\letin{x}{\lambda x . x}{f~f}$$

All the contained program variables
($f$ and $x$) are locally defined variables, so we don't need to provide any further type
information, therefore we call $\W$ with an empty typing context $\Gamma = \emptyset$. 

$$\W(\emptyset, \letin{f}{\lambda x . x}{f~f})$$

The expression matches the case \textbf{(4)}:
\begin{align*}
\W(\emptyset, \letin{f}{\lambda x . x}{f~f}) \defi ~ & (S_2 \circ S_1, \tau_2) \\
\where & (S_1, \tau_1) = \W(\emptyset, \lambda x . x), \\
       & (S_2, \tau_2) = \W(S_1(\emptyset_f),f:\overline{(S_1(\emptyset))}(\tau_1); f~f).
\end{align*}

So we need to first compute the type (and substitution) of the $e_1 = \lambda x . x$, matching the case \textbf{(3)}:
\begin{align*}
\W(\emptyset, \lambda x .x) \defi ~ & (S_1, S_1(\beta_1) \ar \tau_1 ) \\
\where & (S_1, \tau_1) = \W(\{x : \beta_1 \}~,~ x).
\end{align*}

Finally we get to the first variable (case \textbf{(1)}), thus we will get our first result.
\begin{align*}
\W(\{x : \beta_1 \}~,~ x) \defi ~ & ( \{\}, R(\beta_1) ) \where  R = \{\} \\
                              = ~ & ( \{\}, \beta_1 )
\end{align*}

Because $x$ has type $\beta_1$ in the context $\{x : \beta_1\}$, and $\beta_1$ has no universally quantified prefix head, the substitution $R$ is empty, therefore identity. With this information we can get back to computation of $\W(\emptyset, \lambda x .x)$ and finish it. 
\begin{align*}
\W(\emptyset, \lambda x . x) \defi ~ & (S_1, S_1(\beta_1) \ar \tau_1 ) \where (S_1, \tau_1) = ( \{\}, \beta_1 ) \\
 = ~ & (\{\}, \{\}(\beta_1) \ar \beta_1 ) = (\{\}, \beta_1 \ar \beta_1 )
\end{align*}

By this we have finished the first recursive call in computation of \\
$\W(\emptyset, \letin{f}{\lambda x . x}{f~f})$. So we can compute the second recursive call:
\begin{align*}
& \W(S_1(\emptyset_f),f:\overline{(S_1(\emptyset))}(\tau_1); f~f) \where (S_1, \tau_1) = (\{\}, \beta_1 \ar \beta_1 ) \\
= & \W(\{\}(\emptyset_f),f:\overline{(\{\}(\emptyset))}(\beta_1 \ar \beta_1); f~f) \\
= & \W(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}; f~f)
\end{align*}

Now wee need to compute the type of expression $(f~f)$ which is an application (case \textbf{(2)}) from typing context $\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}$, specifying that $f$ has type of the \textit{polymorphic} identity. 
\begin{align*}
\W(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}; f~f) \defi ~ & 
(R \circ S_2 \circ S_1, R(\beta_?) ) \\
\where & (S_1, \tau_1) = \W(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}, f), \\
       & (S_2, \tau_2) = \W(S_1(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}), f), \\
       & R = \Mgu( S_2(\tau_1), \tau_2 \ar \beta_? ).\\
\end{align*}

You can see $\beta_?$ which is used to signify that it is not obvious what index the new fresh variable will have, since the two recursive calls to $\W$ may produce some new fresh variables before $\beta_?$ is introduced. Actually both calls produce one new type variable, thus $\beta_?$ will be $\beta_4$, as we will see.
\begin{align*}
\W(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}, f) \defi ~ & (\{\}, R(\beta_1 \ar \beta_1) ) 
~ \where R = \{\beta_1 \mapsto \beta_2\} \\
= ~ &  (\{\}, \beta_2 \ar \beta_2)
\end{align*}

Now we can continue with the second call:
\begin{align*}
\W(\{\}(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}), f) \defi ~ & (\{\}, R(\beta_1 \ar \beta_1) ) 
~ \where R = \{\beta_1 \mapsto \beta_3\} \\
= ~ &  (\{\}, \beta_3 \ar \beta_3)
\end{align*}

And finally we compute the \textit{most general unification} $R$:
\begin{align*}
R & = \Mgu(\beta_2 \ar \beta_2, (\beta_3 \ar \beta_3) \ar \beta_4 ) \\
  & = \{ \beta_2 \mapsto (\beta_3 \ar \beta_3),~ \beta_4 \mapsto (\beta_3 \ar \beta_3) \}
\end{align*}

One can see that R really unifies $\beta_2 \ar \beta_2$ and $(\beta_3 \ar \beta_3) \ar \beta_4$,
because $R(\beta_2 \ar \beta_2) = (\beta_3 \ar \beta_3) \ar (\beta_3 \ar \beta_3) = R((\beta_3 \ar \beta_3) \ar \beta_4)$. Now the computation of $\W(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}; f~f)$ can be finished:
\begin{align*}
\W(\{f:\forall \beta_1 . \beta_1 \ar \beta_1 \}; f~f) 
\defi ~ & (R \circ S_2 \circ S_1, R(\beta_4) ) \\
    = ~ & (\{ \beta_2 \mapsto (\beta_3 \ar \beta_3), \beta_4 \mapsto (\beta_3 \ar \beta_3) \} \circ \{\} \circ \{\},~ \beta_3 \ar \beta_3 )\\ 
    = ~ & (\{ \beta_2 \mapsto (\beta_3 \ar \beta_3), \beta_4 \mapsto (\beta_3 \ar \beta_3) \}, ~ \beta_3 \ar \beta_3 ) 
\end{align*}

Now we can compute the final result:
\begin{align*}
\W(\emptyset, \letin{f}{\lambda x . x}{f~f}) = ~ & (S_2 \circ S_1, \tau_2) \\
\where & (S_1, \tau_1) = (\{\}, \beta_1 \ar \beta_1 ), \\
       & (S_2, \tau_2) = (\{ \beta_2 \mapsto (\beta_3 \ar \beta_3), \beta_4 \mapsto (\beta_3 \ar \beta_3) \}, ~ \beta_3 \ar \beta_3 ).
\end{align*}

And therefore:
\begin{align*}
\W(\emptyset, \letin{f}{\lambda x . x}{f~f}) = ~ & (\{ \beta_2 \mapsto (\beta_3 \ar \beta_3), \beta_4 \mapsto (\beta_3 \ar \beta_3) \},~ \beta_3 \ar \beta_3)
\end{align*}

We get an unsurprising result:
$$\emptyset \vdash (\letin{f}{\lambda x . x}{f~f}) : \beta_3 \ar \beta_3$$


\section{Inference Rules}

\texttt{TAUT} rule:

$$\unaRule{(x : \sigma) \in \Gamma}{ \Gamma \vdash x : \sigma}$$

\texttt{COMB} rule:

$$\binRule{\Gamma \vdash e_1 : \tau_1 \ar \tau_2}{\Gamma \vdash e_2 : \tau_1}
{\Gamma \vdash (e_1~e_2) : \tau_2}$$

\texttt{ABS} rule:

$$\unaRule{\Gamma_x,x:\tau_1 \vdash e : \tau_2}
{\Gamma \vdash (\lambda x . e) :  \tau_1 \ar \tau_2}$$

\texttt{LET} rule:

$$\binRule{\Gamma \vdash e_1 : \sigma}{\Gamma_x,x:\sigma \vdash e_2 : \tau}
{\Gamma \vdash (\letin{x}{e_1}{e_2}) :  \tau}$$

\texttt{INST} rule:

$$\binRule{\Gamma \vdash e : \sigma}{\sigma \sqsupseteq \sPr}
{\Gamma \vdash e : \sPr}$$

\texttt{GEN} rule:

$$\binRule{\Gamma \vdash e : \sigma}{\alpha \notin \op{FTV}(\Gamma)}
{\Gamma \vdash e : \forall \alpha . \sigma}$$

$\sqsupseteq$ rule:

$$\binRule{\beta_i \notin \op{FTV}(\forall \overline{\alpha}.\tau)}
{\tauPr = \{\overline{\alpha} \mapsto \overline{\tau}\}(\tau)}
{\forall \overline{\alpha}.\tau  ~ \sqsupseteq ~   \forall \overline{\beta}.\tauPr}$$

The same $\sqsupseteq$ rule again, hopefully more readable:

$$\triRule{\beta_i \notin \op{FTV}(\forall \alpha_1\dots\alpha_n.\tau) \text{ for } i \in \{1,\dots,k\}}
{\tauPr = \{ \alpha_1 \mapsto \tau_1,  \dots, \alpha_n \mapsto \tau_n \}(\tau)}
{n,k \geq 0}
{\forall \alpha_1\dots\alpha_n.\tau  ~ \sqsupseteq ~   \forall \beta_1\dots\beta_k.\tauPr}$$



\subsection{Correctness and Completeness of W}

~

\subsubsection{Correctness of W}

If $\W(\Gamma, e) = (S, \tau)$, then exist derivation of the judgment $S(\Gamma) \vdash e : \tau$.

~

\subsubsection{Completeness of W}

Let $\Gamma$ be a context and $e$ a program expression,
and let $\SPr$ be a substitution and $\tauPr$ a type such that:
$ \SPr(\Gamma) \vdash e : \tauPr $, 
then:

(1) $\W(\Gamma,e)$ succeeds (i.e. $\W(\Gamma,e) \neq \bot$), 
let $\W(\Gamma,e) = (S, \tau)$,

(2) there is a substitution $R$ such that $\SPr = R \circ S$ 
and $R\overline{(S(\Gamma))}(\tau) \sqsupseteq \tauPr$. 

~ 

The correctness theorem states that if $\W$ finds a solution, then the solution is correct.
The first part of the completeness theorem, states that if there is a solution, then $\W$ finds one. And the second part formally states that the found solution $(S,\tau)$ is the most general one, by comparing it with an arbitrary solution $(\SPr,\tauPr)$. The substitution $R$ acts as a witness of the fact that we can obtain $\SPr$ by making $S$ more specific ($\SPr = R \circ S$).  

\red{Before it is possible to explain the second part of point two, it is necessary to introduce  the "closure overline" somewhere above.}


\section{Unification Algorithm}
\red{todo: Add some explanation here!}

\Pseudokod{Algorithm finding the most general unification.}
{\textbf{MGU}($\tau_1$, $\tau_2$)}{
	
	$result = \{\}$ \;
	$agenda \la [(\tau_1$, $\tau_2)]$ \;	
	$isOK \la True$ \;

	\;	
	
	\While {agenda not empty $\wedge$ isOK}{
		$(\tau_a, \tau_b) \la agenda.removeFirst()$ \;
		$isOK = \textbf{process}(\tau_a, \tau_b, agenda, result)$			
	}
	\;
	
	\If {$isOK$} {
		\Return $result$
	} \Else {
		\Return $\bot$				
	}
	
}{mguAlg}

\Pseudokod{Processes one type pair.}
{\textbf{process}($\tau_1$, $\tau_2, agenda, result$)}{
	\;
	\If {$\tau_1$ and $\tau_2$ are the same \textbf{TypeVar}} {
		\Return $True$
	} \;

	\If {$\tau_1$ and $\tau_2$ are the same \textbf{TypeSym}} {
		\Return $True$
	} \;

	\If {$\tau_1$ and $\tau_2$ are both \textbf{TypeTerm} with the same length} {
		$agenda.addAll(zip(\tau_1.args(), \tau_2.args()))$ \;
		\Return $True$
	} \;
	
	\If {$\tau_1$ is a \textbf{TypeVar}} {
		\Return $\textbf{processTypeVar}(\tau_1$, $\tau_2, agenda, result)$
	} \;
	
	\If {$\tau_2$ is a \textbf{TypeVar}} {
		\Return $\textbf{processTypeVar}(\tau_2$, $\tau_1, agenda, result)$
	} \;	
	
	\Return $False$
	
}{mguAlgProcess}

\Pseudokod{Processes one $var \mapsto type$ binding.}
{\textbf{processTypeVar}($var, type, agenda, result$)}{
	\;
	
	\If {$type$ contains $var$} {
		\Return $False$
	} \;

	$S \la \{var \mapsto type \}$\;\;

	\For {$entry$ in $result$} {
		$(v \mapsto \tau) \la entry$ \;
		$entry.set_{\tau}( S(\tau) )$
	} \;

	\For {$entry$ in $agenda$} {
		$(\tau_1, \tau_2) \la entry$ \;
		$entry.set_{\tau_1}( S(\tau_1) )$ \;
		$entry.set_{\tau_2}( S(\tau_2) )$
	} \;

	
	$result.add(var \mapsto type)$ \;\;
	
	\Return $True$
	
}{mguAlgProcessTypeVar}
















\backmatter
\end{document}

